import torch
import torch.nn.functional as F
from torch_geometric.nn import GINConv, global_add_pool
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
import networkx as nx

# GIN ëª¨ë¸ ì •ì˜
class GIN(torch.nn.Module):
    """
    Graph Isomorphism Network (GIN)
    Theory: Can distinguish graph structures as powerful as the WL-test.
    Usage: Extract structural embeddings for similarity comparison.
    """
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GIN, self).__init__()
        
        # MLP layers for GIN aggregation
        self.conv1 = GINConv(
            torch.nn.Sequential(
                torch.nn.Linear(in_channels, hidden_channels),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_channels, hidden_channels)
            )
        )
        self.conv2 = GINConv(
            torch.nn.Sequential(
                torch.nn.Linear(hidden_channels, hidden_channels),
                torch.nn.ReLU(),
                torch.nn.Linear(hidden_channels, hidden_channels)
            )
        )
        self.lin = torch.nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch=None):
        # batchê°€ Noneì¼ ê²½ìš° (ë‹¨ì¼ ê·¸ë˜í”„) ì²˜ë¦¬
        if batch is None:
            batch = torch.zeros(x.size(0), dtype=torch.long)

        # 1. Message Passing
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = self.conv2(x, edge_index)
        x = x.relu()

        # 2. Readout (Graph-level Embedding) using Sum Pooling
        # Sum pooling is theoretically better for isomorphism than mean/max
        x = global_add_pool(x, batch)
        
        # 3. Projection
        x = self.lin(x)
        return x

class NeuralGraphMatcher:
    def __init__(self):
        # Feature dimension=1 (Structural only), Hidden=32, Output=16
        self.model = GIN(1, 32, 16)
        self.model.eval() # Inference mode

    def nx_to_pyg(self, G):
        """NetworkX ê·¸ë˜í”„ë¥¼ PyTorch Geometric ë°ì´í„°ë¡œ ë³€í™˜"""
        # ë…¸ë“œ í”¼ì²˜ê°€ ì—†ìœ¼ë¯€ë¡œ ëª¨ë“  ë…¸ë“œì— ìƒìˆ˜ 1 ë¶€ì—¬ (êµ¬ì¡°ë§Œ ë³´ê² ë‹¤ëŠ” ì˜ë¯¸)
        for i in G.nodes():
            G.nodes[i]['x'] = [1.0]
            
        data = from_networkx(G)
        # PyGì˜ x(feature) í…ì„œ í™•ì¸ ë° ì°¨ì› ë§ì¶¤
        if data.x is None:
             data.x = torch.ones((G.number_of_nodes(), 1))
        else:
             data.x = data.x.view(-1, 1).float()
             
        return data

    def calculate_similarity(self, G1, G2):
        """
        ë‘ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ìœ ì‚¬ë„(Cosine Similarity) ê³„ì‚°
        """
        data1 = self.nx_to_pyg(G1)
        data2 = self.nx_to_pyg(G2)
        
        with torch.no_grad():
            emb1 = self.model(data1.x, data1.edge_index)
            emb2 = self.model(data2.x, data2.edge_index)
            
            similarity = F.cosine_similarity(emb1, emb2)
            return similarity.item()

if __name__ == "__main__":
    matcher = NeuralGraphMatcher()
    
    # Case 1: ì™„ë²½í•˜ê²Œ ë™ì¼í•œ êµ¬ì¡° (Isomorphic)
    G_pattern = nx.cycle_graph(5) # 5ê°í˜• ê³ ë¦¬
    G_suspect = nx.cycle_graph(5) # 5ê°í˜• ê³ ë¦¬
    
    score_iso = matcher.calculate_similarity(G_pattern, G_suspect)
    print(f"ğŸ¤– Similarity (Isomorphic): {score_iso:.4f}") # 1.0ì— ê°€ê¹Œì›Œì•¼ í•¨

    # Case 2: ì•½ê°„ ë‹¤ë¥¸ êµ¬ì¡° (Non-Isomorphic but similar)
    G_noise = nx.cycle_graph(5)
    G_noise.add_edge(0, 2) # ì—£ì§€ í•˜ë‚˜ ì¶”ê°€ (ë…¸ì´ì¦ˆ)
    
    score_noise = matcher.calculate_similarity(G_pattern, G_noise)
    print(f"ğŸ¤– Similarity (Noisy): {score_noise:.4f}") # 1.0ë³´ë‹¤ ë‚®ì•„ì•¼ í•¨
    
    # Case 3: ì™„ì „íˆ ë‹¤ë¥¸ êµ¬ì¡°
    G_diff = nx.star_graph(4) # ë³„ ëª¨ì–‘
    
    score_diff = matcher.calculate_similarity(G_pattern, G_diff)
    print(f"ğŸ¤– Similarity (Different): {score_diff:.4f}") # í›¨ì”¬ ë‚®ì•„ì•¼ í•¨
